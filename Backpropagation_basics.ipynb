{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac50351c",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "Fonctionnement d'un RNN élémentaire (2x1) il prend en entrée deux inputs et sort un output.\n",
    "\n",
    "Backpropagation : calcul et ajustement des weights à chaque itération. On optimise pour réduire l'écart entre les valeurs prédites et les valeurs réelles.  \n",
    "\n",
    "Ce fichier reproduit l'algo de backpropagation sur un RNN simple dont  l'architecture est la suivante : \n",
    "hidden layer : 2x2\n",
    "output layer : 2x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90803f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version is  2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"tf version is \", tf.__version__)\n",
    "\n",
    "tf.keras.backend.set_floatx('float64') # on travaille avec des flottants uniquement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197af5ab",
   "metadata": {},
   "source": [
    "### construction du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0714dc62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class model(tf.keras.Model) :\n",
    "    \n",
    "    def __init__(self, x, y) :\n",
    "        super(model, self).__init__()\n",
    "        \n",
    "        # input layer : deux inputs constants\n",
    "        self.x_input = tf.constant(x, dtype = tf.float64)\n",
    "        self.y_input = tf.constant(y, dtype = tf.float64) \n",
    "        \n",
    "        # layer 1 - hidden layer\n",
    "        self.w1 = tf.Variable(tf.random.uniform([2,2], \n",
    "                                                minval = 0., \n",
    "                                                maxval = 1., \n",
    "                                                dtype = tf.float64, \n",
    "                                                trainable = True,\n",
    "                                               name = 'w1')) # matrice des poids : 2 neuronnes - 2 poids par neuronne ; trainable = T ie utilisable dans la méthode du gradien descendant\n",
    "        \n",
    "        self.b1 = tf.Variable(tf.random.uniform([2], \n",
    "                                                minval = 0., \n",
    "                                                maxval = 1., \n",
    "                                                dtype = tf.float64, \n",
    "                                                trainable = True,\n",
    "                                               name = 'b1')) # vecteur de biais sur les poids \n",
    "        \n",
    "        # layer 2 - output layer\n",
    "        self.w2 = tf.Variable(tf.random.uniform([2,1], \n",
    "                                                minval = 0., \n",
    "                                                maxval = 1., \n",
    "                                                dtype = tf.float64, \n",
    "                                                trainable = True,\n",
    "                                               name = 'w2')) # matrice des poids : 2 neuronnes - 2 poids par neuronne ; trainable = T ie utilisable dans la méthode du gradien descendant\n",
    "        \n",
    "        self.b2 = tf.Variable(tf.random.uniform([1], \n",
    "                                                minval = 0., \n",
    "                                                maxval = 1., \n",
    "                                                dtype = tf.float64, \n",
    "                                                trainable = True,\n",
    "                                               name = 'b2')) # vecteur de biais sur les poids \n",
    "    def call(self, inputs) :\n",
    "        # partie calculs\n",
    "        inputs = tf.constant(x, dtype = tf.float64)\n",
    "        \n",
    "        in_neurons_hidden_layer = tf.add(tf.linalg.matmul(inputs, self.w1), self.b1) # x*w + biais\n",
    "        out_neurons_hidden_layer = tf.sigmoid(in_neurons_hidden_layer) # on applique une fonction d'activation sigmoide au résultat \n",
    "\n",
    "        in_neurons_output_layer = tf.add(tf.linalg.matmul(out_neurons_hidden_layer, self.w2), self.b2) # prend en entrée l'output de la couche hidden précédente + fais le calcul\n",
    "        out_neurons_output_layer = tf.sigmoid(in_neurons_output_layer) # on applique une fonction d'activation sigmoide au résultat \n",
    "\n",
    "        return out_neurons_output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f862eef",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05283f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# creation d'un fichier de récap pour tensorboard\n",
    "current_time = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "logdir = 'logs/gradient_tape/'+ current_time + '/train'\n",
    "summary_writer = tf.summary.create_file_writer(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "522390b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input1</th>\n",
       "      <th>input2</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   input1  input2  output\n",
       "0     0.0     0.0     1.0\n",
       "1     1.0     1.0     0.0\n",
       "2     0.0     1.0     1.0\n",
       "3     1.0     0.0     1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XOR : illustration du fonctionnement du RNN\n",
    "\n",
    "x = np.array([[0.,0.], [1.,1.], [0.,1.], [1.,0.]])\n",
    "y = np.array([1.,0.,1.,1.])\n",
    "\n",
    "pd.DataFrame({\n",
    "    'input1' : np.vstack(x).T[0],\n",
    "    'input2' : np.vstack(x).T[1],\n",
    "    'output' : y\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6654268a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Got an unexpected keyword argument 'trainable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-3bb88f87016e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# backpropagation phase :: descente du gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# loss function : calcule le delta entre la pred du modele et la valeur attendue et optimise les poids pour réduire le delta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# optimizer du (stochastic) gradient descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-28d21b1c4f7b>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# layer 1 - hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         self.w1 = tf.Variable(tf.random.uniform([2,2], \n\u001b[0m\u001b[0;32m     12\u001b[0m                                                 \u001b[0mminval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                                 \u001b[0mmaxval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1074\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miterable_params\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m           \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplace_iterable_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1076\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi_dispatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1077\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Got an unexpected keyword argument 'trainable'"
     ]
    }
   ],
   "source": [
    "# backpropagation phase :: descente du gradient \n",
    "# loss function : calcule le delta entre la pred du modele et la valeur attendue et optimise les poids pour réduire le delta\n",
    "model = model(x, y)\n",
    "optimizer = tf.optimizers.SGD(learning_rate = 0.01) # optimizer du (stochastic) gradient descent\n",
    "\n",
    "def loss(outputs_models, targets) :\n",
    "    \n",
    "    error = tf.math.subtract(targets, outputs_models)\n",
    "    return tf.reduce_sum(tf.square(error))\n",
    "\n",
    "\n",
    "def get_gradient(model, inputs, targets) :\n",
    "    \n",
    "    with tf.GradientsTape(watch_accessed_variables = True) as tape : \n",
    "        loss_value = loss(model(inputs), targets)\n",
    "    \n",
    "    return tape.gradient(loss_value, [model.w1, model.b1, model.w2, model.b2])\n",
    "\n",
    "\n",
    "def run_network(inputs, targets, epochs) : \n",
    "    \n",
    "    for i in range(epochs) :\n",
    "        grads = get_gradient(model, inputs, targets)\n",
    "        optimizer.apply_gradients(zip(grads, [model.w1, model.w2, model.b1, model.b2]))\n",
    "        loss_epoch = loss(model(inputs), model.y_input)\n",
    "        \n",
    "        with summary_writer.as_default() :\n",
    "            tf.summary_writer('loss', loss_epochs, step=i)\n",
    "        if i % 100 == 0 :\n",
    "            print('Loss at the epoch ',{i}, ':', {loss_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc3d88f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class zip in module builtins:\n",
      "\n",
      "class zip(object)\n",
      " |  zip(*iterables) --> A zip object yielding tuples until an input is exhausted.\n",
      " |  \n",
      " |     >>> list(zip('abcdefg', range(3), range(4)))\n",
      " |     [('a', 0, 0), ('b', 1, 1), ('c', 2, 2)]\n",
      " |  \n",
      " |  The zip object yields n-length tuples, where n is the number of iterables\n",
      " |  passed as positional arguments to zip().  The i-th element in every tuple\n",
      " |  comes from the i-th iterable argument to zip().  This continues until the\n",
      " |  shortest argument is exhausted.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __next__(self, /)\n",
      " |      Implement next(self).\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Return state information for pickling.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f445ca66",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-222a7f6e237c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# compilation avec la fonction run neetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrun_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'run_network' is not defined"
     ]
    }
   ],
   "source": [
    "# compilation avec la fonction run neetwork\n",
    "run_network(x, y, epochs = 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### tensorboad - evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b889d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
