{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac50351c",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "Fonctionnement d'un RNN élémentaire (2x1) il prend en entrée deux inputs et sort un output.\n",
    "\n",
    "Backpropagation : calcul et ajustement des weights à chaque itération. On optimise pour réduire l'écart entre les valeurs prédites et les valeurs réelles.  \n",
    "\n",
    "Ce fichier reproduit l'algo de backpropagation sur un RNN simple dont  l'architecture est la suivante : \n",
    "hidden layer : 2x2\n",
    "output layer : 2x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90803f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version is  2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"tf version is \", tf.__version__)\n",
    "\n",
    "tf.keras.backend.set_floatx('float64') # on travaille avec des flottants uniquement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197af5ab",
   "metadata": {},
   "source": [
    "### construction du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a2cf276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(tf.keras.Model) :\n",
    "    \n",
    "    def __init__(self, x, y) :\n",
    "        super(model, self).__init__()\n",
    "        \n",
    "        # input layer : deux inputs constants\n",
    "        self.x_input = tf.constant(x, dtype = tf.float64)\n",
    "        self.y_input = tf.constant(y, dtype = tf.float64) \n",
    "        \n",
    "        # layer 1 - hidden layer\n",
    "        self.w1 = tf.Variable(tf.random.uniform([2,2], \n",
    "                                                minval = 0., \n",
    "                                                maxval = 1., \n",
    "                                                dtype = tf.float64), \n",
    "                                               name = 'w1') # matrice des poids : 2 neuronnes - 2 poids par neuronne ; trainable = T ie utilisable dans la méthode du gradient descendant\n",
    "        \n",
    "        self.b1 = tf.Variable(tf.random.uniform([2], \n",
    "                                                minval = 0., \n",
    "                                                maxval = 1., \n",
    "                                                dtype = tf.float64), \n",
    "                                               name = 'b1') # vecteur de biais sur les poids \n",
    "        \n",
    "        # layer 2 - output layer\n",
    "        self.w2 = tf.Variable(tf.random.uniform([2,1], \n",
    "                                                minval = 0., \n",
    "                                                maxval = 1., \n",
    "                                                dtype = tf.float64), \n",
    "                                               name = 'w2') # matrice des poids : 2 neuronnes - 2 poids par neuronne ; trainable = T ie utilisable dans la méthode du gradient descendant\n",
    "        \n",
    "        self.b2 = tf.Variable(tf.random.uniform([1], \n",
    "                                                minval = 0., \n",
    "                                                maxval = 1., \n",
    "                                                dtype = tf.float64), \n",
    "                                               name = 'b2') # vecteur de biais sur les poids \n",
    "    def call(self, inputs) :\n",
    "        # partie calculs\n",
    "        inputs = tf.constant(x, dtype = tf.float64)\n",
    "        \n",
    "        in_neurons_hidden_layer = tf.add(tf.linalg.matmul(inputs, self.w1), self.b1) # x*w + biais\n",
    "        out_neurons_hidden_layer = tf.sigmoid(in_neurons_hidden_layer) # on applique une fonction d'activation sigmoide au résultat \n",
    "\n",
    "        in_neurons_output_layer = tf.add(tf.linalg.matmul(out_neurons_hidden_layer, self.w2), self.b2) # prend en entrée l'output de la couche hidden précédente + fais le calcul\n",
    "        out_neurons_output_layer = tf.sigmoid(in_neurons_output_layer) # on applique une fonction d'activation sigmoide au résultat \n",
    "\n",
    "        return out_neurons_output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f862eef",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "522390b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input1</th>\n",
       "      <th>input2</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   input1  input2  output\n",
       "0     0.0     0.0     1.0\n",
       "1     1.0     1.0     0.0\n",
       "2     0.0     1.0     1.0\n",
       "3     1.0     0.0     1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "\n",
    "# XOR : illustration du fonctionnement du RNN\n",
    "\n",
    "x = np.array([[0.,0.], [1.,1.], [0.,1.], [1.,0.]])\n",
    "y = np.array([1.,0.,1.,1.])\n",
    "\n",
    "pd.DataFrame({\n",
    "    'input1' : np.vstack(x).T[0],\n",
    "    'input2' : np.vstack(x).T[1],\n",
    "    'output' : y\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6654268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# efface les logs des apprentissages passés\n",
    "#!rm -rf ./logs/\n",
    "# creation d'un fichier de récap pour tensorboard\n",
    "current_time = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "logdir = 'logs/gradient_tape/'+ current_time + '/train'\n",
    "summary_writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "# backpropagation phase :: descente du gradient \n",
    "# loss function : calcule le delta entre la pred du modele et la valeur attendue et optimise les poids pour réduire le delta\n",
    "model = model(x, y)\n",
    "optimizer = tf.optimizers.SGD(learning_rate = 0.1) # optimizer du (stochastic) gradient descent\n",
    "\n",
    "def loss(outputs_model, targets) :\n",
    "    # perte = ecart entre le target et l'output du modele \n",
    "    error = tf.math.subtract(targets, outputs_model)\n",
    "    return tf.reduce_sum(tf.square(error)) # on choisit de retourner l'erreur quadratique ici par exemple\n",
    "\n",
    "\n",
    "def get_gradient(model, inputs, targets) :\n",
    "    \n",
    "    with tf.GradientTape() as tape : \n",
    "        loss_value = loss(model(inputs), targets)\n",
    "    \n",
    "    return tape.gradient(loss_value, [model.w1, model.b1, model.w2, model.b2])\n",
    "\n",
    "\n",
    "def run_network(inputs, targets, epochs) : \n",
    "    \n",
    "    for i in range(epochs) :\n",
    "        grads = get_gradient(model, inputs, targets)\n",
    "        optimizer.apply_gradients(zip(grads, [model.w1, model.b1, model.w2, model.b2]))\n",
    "        loss_epoch = loss(model(inputs), model.y_input)\n",
    "        \n",
    "        with summary_writer.as_default() :\n",
    "           tf.summary.scalar('loss', loss_epoch, step=i)\n",
    "        if i % 100 == 0 :\n",
    "            print(f\"Loss at the epoch {i} : {loss_epoch}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc3d88f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#help(zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f445ca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at the epoch 0 : 3.0327095116222766.\n",
      "Loss at the epoch 100 : 3.002374584972741.\n",
      "Loss at the epoch 200 : 3.0019573106929176.\n",
      "Loss at the epoch 300 : 3.001632303538483.\n",
      "Loss at the epoch 400 : 3.0013743135725877.\n",
      "Loss at the epoch 500 : 3.0011662473654326.\n",
      "Loss at the epoch 600 : 3.0009961819222744.\n",
      "Loss at the epoch 700 : 3.000855584785963.\n",
      "Loss at the epoch 800 : 3.0007382124621236.\n",
      "Loss at the epoch 900 : 3.0006394062160773.\n"
     ]
    }
   ],
   "source": [
    "# compilation avec la fonction run neetwork\n",
    "run_network(x, y, epochs = 1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe5010b",
   "metadata": {},
   "source": [
    "### tensorboad - post evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b889d78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cb6d139",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6816), started 0:05:51 ago. (Use '!kill 6816' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c11fa05f6b465842\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c11fa05f6b465842\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c86459dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tb version is  2.10.0a20220807\n"
     ]
    }
   ],
   "source": [
    "import tensorboard as tb\n",
    "print(\"tb version is \", tb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f79303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard-plugin-wit 1.8.1\n",
      "tb-nightly 2.10.0a20220807\n"
     ]
    }
   ],
   "source": [
    "# combien de versions de tensorboard installées ?\n",
    "import pkg_resources\n",
    "\n",
    "for entry_point in pkg_resources.iter_entry_points('tensorboard_plugins'):\n",
    "    print(entry_point.dist)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
